{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '\\\\Users\\\\liamh\\\\OneDrive - University of Strathclyde\\\\University'\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "\n",
    "# Network Architecture\n",
    "num_hidden = 350\n",
    "num_outputs = 10\n",
    "num_steps = 25\n",
    "dropout = 0.25\n",
    "\n",
    "# Temporal Dynamics\n",
    "time_step = 1e-3\n",
    "tau_mem = 2e-2\n",
    "beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_in_data(res, ratio = 1):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((res, res)), #Resize images to 28*28\n",
    "        transforms.Grayscale(), # Make sure image is grayscale\n",
    "        transforms.ToTensor()]) # change each image array to a tensor which automatically scales inputs to [0,1]\n",
    "\n",
    "    mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # Download training set and apply transformations. \n",
    "    mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform) # same for test set\n",
    "\n",
    "    train_len = int(len(mnist_train)/ratio)\n",
    "    dummy_len = len(mnist_train) - train_len\n",
    "    train_dataset, _ = random_split(mnist_train, (train_len, dummy_len), generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Load the data into the DataLoader so it's passed through the model, shuffled in batches. \n",
    "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def output_formula(input_size, filter_size, padding, stride):\n",
    "    formula = math.floor(((((input_size - filter_size + 2*padding)/stride) + 1)))\n",
    "    \n",
    "    return formula \n",
    "\n",
    "def all_output_sizes(res, conv_filter = 3, conv_padding = 1, conv_stride = 1, mp_filter = 3, mp_padding = 0, mp_stride = 2):\n",
    "    \n",
    "    conv1 = output_formula(res, conv_filter, conv_padding, conv_stride)   # Output size from applying conv1 to input \n",
    "    mp1 = output_formula(conv1, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 1 to conv1 \n",
    "    \n",
    "    conv2 = output_formula(mp1, conv_filter, conv_padding, conv_stride)   # Output size from applying conv2 to max pooling 1\n",
    "    conv3 = output_formula(conv2, conv_filter, conv_padding, conv_stride) # Output size from applying conv3 to conv 2\n",
    "    mp2 = output_formula(conv3, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 2 to conv3\n",
    "    \n",
    "    conv4 = output_formula(mp2, conv_filter, conv_padding, conv_stride)   # Output size from applying conv 4 to max pooling 2\n",
    "    conv5 = output_formula(conv4, conv_filter, conv_padding, conv_stride) # Output size from applying conv5 to conv 4\n",
    "    mp3 = output_formula(conv5, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 3 to conv 5\n",
    "    \n",
    "    outputs_I_need = [mp1, conv2, mp2, conv4, mp3]\n",
    "    \n",
    "    return outputs_I_need\n",
    "\n",
    "def plot_training_history(history, res, loss_upper = 1.05, acc_lower = -0.05, acc_higher = 105):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    \n",
    "    ax1.plot(history['avg_train_loss'], label='train loss',marker = 'o')\n",
    "    ax1.plot(history['avg_valid_loss'], label='validation loss',marker = 'o')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax1.set_ylim([-0.05, loss_upper])\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Loss',fontsize = 16)\n",
    "    ax1.set_xlabel('Epoch',fontsize = 16)\n",
    "    \n",
    "    ax2.plot(history['train_accuracy'], label='train accuracy',marker = 'o')\n",
    "    ax2.plot(history['valid_accuracy'], label='validation accuracy',marker = 'o')\n",
    "\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2.set_ylim([acc_lower, acc_higher])\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    ax2.set_ylabel('Accuracy',fontsize = 16)\n",
    "    ax2.yaxis.set_major_formatter(PercentFormatter(100))\n",
    "    ax2.set_xlabel('Epoch',fontsize = 16)\n",
    "    fig.suptitle(f'Training history ({res}*{res})',fontsize = 20)\n",
    "    plt.show()\n",
    "\n",
    "def store_best_results(history):\n",
    "    # Want to take the last entry from each output(best results) and store them all in a Dataframe\n",
    "    placeholder = []\n",
    "    placeholder.append(history['avg_train_loss'][-1])\n",
    "    placeholder.append(history['train_accuracy'][-1])\n",
    "    placeholder.append(history['avg_valid_loss'][-1])\n",
    "    placeholder.append(history['valid_accuracy'][-1])\n",
    "    \n",
    "    return placeholder\n",
    "\n",
    "def put_results_in_df(output):\n",
    "    df = pd.DataFrame()\n",
    "    df['avg_train_loss'] = output['avg_train_loss']\n",
    "    df['train_accuracy'] = output['train_accuracy']\n",
    "    df['avg_valid_loss'] = output['avg_valid_loss']\n",
    "    df['valid_accuracy'] = output['valid_accuracy']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiking CNN and training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a different network\n",
    "class CSNN(nn.Module):\n",
    "    def __init__(self,spike_grad):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = conv_kernel_size, padding = conv_padding_size) # Do I change channels to a variable incase I end up with RGB images? ## Padding = 0 as all information is at the centre of image (may change if lower resolution)\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size = mp_kernel_size, stride = mp_stride_length)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = conv_kernel_size, padding = conv_padding_size)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = conv_kernel_size, padding = conv_padding_size)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size = mp_kernel_size, stride = mp_stride_length)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = conv_kernel_size, padding = conv_padding_size)\n",
    "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = conv_kernel_size, padding = conv_padding_size)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = mp_kernel_size, stride = mp_stride_length)\n",
    "        self.lif5 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * output_sizes[-1] * output_sizes[-1], num_hidden)\n",
    "        self.lif6 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
    "        self.lif7 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.fc3 = nn.Linear(num_hidden, num_outputs) \n",
    "        self.lif8 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        spk1, mem1 = self.lif1.init_leaky(batch_size, 64, output_sizes[0], output_sizes[0])\n",
    "        spk2, mem2 = self.lif2.init_leaky(batch_size, 128, output_sizes[1], output_sizes[1])\n",
    "        spk3, mem3 = self.lif3.init_leaky(batch_size, 128, output_sizes[2], output_sizes[2])\n",
    "        spk4, mem4 = self.lif4.init_leaky(batch_size, 256, output_sizes[3], output_sizes[3])\n",
    "        \n",
    "        spk5, mem5 = self.lif5.init_leaky(batch_size, 256, output_sizes[-1], output_sizes[-1])\n",
    "        \n",
    "        spk6, mem6 = self.lif6.init_leaky(batch_size, num_hidden)\n",
    "        spk7, mem7 = self.lif7.init_leaky(batch_size, num_hidden)\n",
    "        \n",
    "        spk8, mem8 = self.lif8.init_leaky(batch_size, num_outputs)\n",
    "        \n",
    "        spk8_rec = []\n",
    "        mem8_rec = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.mp1(self.conv1(x[step]))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            \n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.conv3(spk2)\n",
    "            cur3 = self.mp2(cur3)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            \n",
    "            cur4 = self.conv4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            cur5 = self.conv5(spk4)\n",
    "            cur5 = self.maxpool(cur5)\n",
    "            spk5, mem5 = self.lif5(cur5, mem5)\n",
    "            \n",
    "            spk5 = self.drop1(spk5)\n",
    "            cur6 = self.fc1(spk5.view(batch_size, -1))\n",
    "            spk6, mem6 = self.lif6(cur6, mem6)\n",
    "            \n",
    "            spk6 = self.drop2(spk6)\n",
    "            cur7 = self.fc2(spk6)\n",
    "            spk7, mem7 = self.lif7(cur7, mem7)\n",
    "            \n",
    "            cur8 = self.fc3(spk7)\n",
    "            spk8, mem8 = self.lif8(cur8, mem8)\n",
    "            \n",
    "            spk8_rec.append(spk8)\n",
    "            mem8_rec.append(mem8)\n",
    "            \n",
    "        return torch.stack(spk8_rec, dim=0), torch.stack(mem8_rec, dim=0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rate_spiking_mse_model(resolution, train_loader, valid_loader, model, epochs ,device = device, verbose = True):\n",
    "    start_time = time.time()\n",
    "    print('Starting Training')\n",
    "    history = defaultdict(list)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999)) # Just an Adam Optimiser\n",
    "    \n",
    "    # Training variables\n",
    "    train_size = len(train_loader.dataset)\n",
    "    train_num_batches = len(train_loader)\n",
    "    \n",
    "    # validation variables\n",
    "    valid_size = len(valid_loader.dataset)\n",
    "    valid_num_batches = len(valid_loader)\n",
    "    \n",
    "    \n",
    "    for t in range(epochs):\n",
    "        \n",
    "        avg_train_loss = 0\n",
    "        correct = 0\n",
    "        avg_valid_loss, valid_correct = 0, 0\n",
    "        model.train()\n",
    "        for batch, (data_it, targets_it) in enumerate(train_loader):\n",
    "            data_it = data_it.to(device)\n",
    "            targets_it = targets_it.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute prediction and loss\n",
    "            spike_data = spikegen.rate(data_it, num_steps=num_steps, gain=1, offset=0)\n",
    "            spk_targets_it = torch.clamp(spikegen.to_one_hot(targets_it, 10) * 1.05, min=0.05)\n",
    "            \n",
    "            spk_rec, mem_rec = model(spike_data.view(num_steps,batch_size, 1,resolution,resolution)) \n",
    "            \n",
    "            # Sum loss over time steps: BPTT\n",
    "            loss = torch.zeros((1), dtype=dtype, device=device)   # creates a 1D tensor to store total loss over time. \n",
    "            for step in range(num_steps):\n",
    "                loss += loss_fn(mem_rec[step], spk_targets_it) # Loss at each time step is added to give total loss.\n",
    "\n",
    "            avg_train_loss += loss\n",
    "            \n",
    "            _, predicted = spk_rec.sum(dim=0).max(1) \n",
    "            correct += (predicted == targets_it).type(torch.float).sum().item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss /= train_num_batches\n",
    "        accuracy = correct / train_size * 100      \n",
    "        history['avg_train_loss'].append(avg_train_loss.item())\n",
    "        history['train_accuracy'].append(accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Epoch {t+1} of {epochs}\")\n",
    "            print('-' * 15)\n",
    "            print(f\"Training Results, Epoch {t+1}:\\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "\n",
    "              ###################### VALIDATION LOOP ##############################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for valid_data_it, valid_targets_it in valid_loader:\n",
    "                valid_data_it = valid_data_it.to(device)\n",
    "                valid_targets_it = valid_targets_it.to(device)\n",
    "                \n",
    "                valid_spike_data = spikegen.rate(valid_data_it, num_steps=num_steps, gain=1, offset=0)\n",
    "                valid_spk_targets_it = torch.clamp(spikegen.to_one_hot(targets_it, 10) * 1.05, min=0.05)\n",
    "\n",
    "                valid_spk_rec, valid_mem_rec = model(valid_spike_data.view(num_steps,batch_size, 1, resolution, resolution)) \n",
    "                \n",
    "                valid_loss = torch.zeros((1),dtype = dtype, device = device)    \n",
    "                for step in range(num_steps):\n",
    "                    valid_loss += loss_fn(valid_mem_rec[step], valid_spk_targets_it)\n",
    "                \n",
    "                avg_valid_loss += valid_loss\n",
    "                \n",
    "                \n",
    "                _, valid_predicted = valid_spk_rec.sum(dim=0).max(1)\n",
    "                valid_correct += (valid_predicted == valid_targets_it).type(torch.float).sum().item()\n",
    "        \n",
    "\n",
    "        avg_valid_loss /= valid_num_batches\n",
    "        valid_accuracy = valid_correct / valid_size * 100\n",
    "              \n",
    "        history['avg_valid_loss'].append(avg_valid_loss.item())\n",
    "        history['valid_accuracy'].append(valid_accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Validation Results, Epoch {t+1}: \\n Accuracy: {(valid_accuracy):>0.1f}%, Avg loss: {avg_valid_loss.item():>8f} \\n\")\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Final Train Accuracy: {(accuracy):>0.1f}%, and Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "    print(f\"Final Validation Accuracy: {(valid_accuracy):>0.1f}%, and Avg loss: {avg_valid_loss.item():>8f} \\n\")\n",
    "    current_time = time.time()\n",
    "    total = current_time - start_time\n",
    "    print(f'Training time: {round(total/60,2)} minutes')\n",
    "    return history\n",
    "\n",
    "def get_rate_mse_snn_results(resolution,epochs = 20,ratio = 1, slope = 25, verbose = True):\n",
    "    spike_grad = surrogate.fast_sigmoid(slope = slope)\n",
    "    train, valid = load_in_data(resolution, ratio)\n",
    "    model = CSNN(spike_grad).to(device)\n",
    "\n",
    "    output = train_rate_spiking_mse_model(resolution, train,valid,model,epochs, verbose = verbose)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56x56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: These models were run on Google Colab due to GPU constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel_size = 3\n",
    "conv_stride_length = 1 \n",
    "conv_padding_size = 1\n",
    "mp_kernel_size = 2 \n",
    "mp_stride_length = 2 \n",
    "mp_padding_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sizes = all_output_sizes(56, conv_kernel_size, conv_padding_size, conv_stride_length, mp_kernel_size, mp_padding_size, mp_stride_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_56_r1 = get_rate_mse_snn_results(resolution = 56, ratio = 1, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_56_r4 = get_rate_mse_snn_results(resolution = 56, ratio = 4, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_56_r10 = get_rate_mse_snn_results(resolution = 56, ratio = 10, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_56_r100 = get_rate_mse_snn_results(resolution = 56, ratio = 100, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sizes = all_output_sizes(28, conv_kernel_size, conv_padding_size, conv_stride_length, mp_kernel_size, mp_padding_size, mp_stride_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.850419 \n",
      "\n",
      "Final Validation Accuracy: 99.3%, and Avg loss: 3.886898 \n",
      "\n",
      "Training time: 436.13 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_28_r1 = get_rate_mse_snn_results(resolution = 28, ratio = 1, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.846046 \n",
      "\n",
      "Final Validation Accuracy: 99.0%, and Avg loss: 3.835765 \n",
      "\n",
      "Training time: 157.47 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_28_r4 = get_rate_mse_snn_results(resolution = 28, ratio = 4, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.870913 \n",
      "\n",
      "Final Validation Accuracy: 98.7%, and Avg loss: 3.864564 \n",
      "\n",
      "Training time: 89.04 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_28_r10 = get_rate_mse_snn_results(resolution = 28, ratio = 10, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 96.0%, and Avg loss: 1.014356 \n",
      "\n",
      "Final Validation Accuracy: 96.4%, and Avg loss: 3.751583 \n",
      "\n",
      "Training time: 42.8 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_28_r100 = get_rate_mse_snn_results(resolution = 28, ratio = 100, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14x14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel_size = 3\n",
    "conv_stride_length = 1 \n",
    "conv_padding_size = 1\n",
    "mp_kernel_size = 3\n",
    "mp_stride_length = 1\n",
    "mp_padding_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sizes = all_output_sizes(14, conv_kernel_size, conv_padding_size, conv_stride_length, mp_kernel_size, mp_padding_size, mp_stride_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 98.2%, and Avg loss: 1.008348 \n",
      "\n",
      "Final Validation Accuracy: 98.1%, and Avg loss: 3.716639 \n",
      "\n",
      "Training time: 670.86 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_14_r1 = get_rate_mse_snn_results(resolution = 14, ratio = 1, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 98.8%, and Avg loss: 0.987170 \n",
      "\n",
      "Final Validation Accuracy: 97.7%, and Avg loss: 3.735347 \n",
      "\n",
      "Training time: 159.39 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_14_r4 = get_rate_mse_snn_results(resolution = 14, ratio = 4, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 98.2%, and Avg loss: 1.019498 \n",
      "\n",
      "Final Validation Accuracy: 97.4%, and Avg loss: 3.701958 \n",
      "\n",
      "Training time: 111.47 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_14_r10 = get_rate_mse_snn_results(resolution = 14, ratio = 10, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 94.8%, and Avg loss: 1.151304 \n",
      "\n",
      "Final Validation Accuracy: 92.6%, and Avg loss: 3.596613 \n",
      "\n",
      "Training time: 49.8 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_14_r100 = get_rate_mse_snn_results(resolution = 14, ratio = 100, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel_size = 3\n",
    "conv_stride_length = 1 \n",
    "conv_padding_size = 1\n",
    "mp_kernel_size = 2 \n",
    "mp_stride_length = 1\n",
    "mp_padding_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sizes = all_output_sizes(7, conv_kernel_size, conv_padding_size, conv_stride_length, mp_kernel_size, mp_padding_size, mp_stride_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 5, 5, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 85.7%, and Avg loss: 1.518860 \n",
      "\n",
      "Final Validation Accuracy: 86.7%, and Avg loss: 3.206643 \n",
      "\n",
      "Training time: 236.97 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_7_r1 = get_rate_mse_snn_results(resolution = 7, ratio = 1, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 84.5%, and Avg loss: 1.475704 \n",
      "\n",
      "Final Validation Accuracy: 83.9%, and Avg loss: 3.258993 \n",
      "\n",
      "Training time: 104.75 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_7_r4 = get_rate_mse_snn_results(resolution = 7, ratio = 4, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 81.2%, and Avg loss: 1.507865 \n",
      "\n",
      "Final Validation Accuracy: 81.0%, and Avg loss: 3.231734 \n",
      "\n",
      "Training time: 62.57 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_7_r10 = get_rate_mse_snn_results(resolution = 7, ratio = 10, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 70.7%, and Avg loss: 1.716163 \n",
      "\n",
      "Final Validation Accuracy: 69.6%, and Avg loss: 3.029347 \n",
      "\n",
      "Training time: 23.58 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_7_r100 = get_rate_mse_snn_results(resolution = 7, ratio = 100, epochs = 75, slope = 5, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_res = ['output_28', 'output_14','output_7']\n",
    "output_ratio = ['_r1','_r4','_r10','_r100']\n",
    "index = ['avg_train_loss', 'train_accuracy', 'avg_valid_loss', 'valid_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = []\n",
    "all_models = []\n",
    "for name in output_res:\n",
    "    for ratio in output_ratio: \n",
    "        model_name = name + ratio\n",
    "        all_models.append(model_name)\n",
    "        for indice in index: \n",
    "            column_name = name + ratio + '_' + indice\n",
    "            all_columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for entry in all_models:\n",
    "    for key in index:\n",
    "        string = entry + '_' + key\n",
    "        \n",
    "        df[string] = locals()[entry][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_scnn_training_histories_updated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
