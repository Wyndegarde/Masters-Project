{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import spikegen\n",
    "import snntorch.spikeplot as splt\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_path='/data/mnist'\n",
    "data_path = '\\\\Users\\\\liamh\\\\OneDrive - University of Strathclyde\\\\University'\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "\n",
    "# Network Architecture\n",
    "num_hidden = 350\n",
    "num_outputs = 10\n",
    "num_steps = 25\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.NLLLoss()  # Negative log-likelihood loss function\n",
    "log_softmax_fn = nn.LogSoftmax(dim=-1) # Softmax activation for the output layer. -1 in 'dim' indicates last dimension (the labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Ann_Net(nn.Module):\n",
    "    def __init__(self,res):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(res*res, num_hidden)     # input layer with as many neurons as pixels. \n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)    # Second Dense/linear layer that receives the output spikes from previous layer\n",
    "        self.fc3 = nn.Linear(num_hidden, num_outputs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return(F.log_softmax(self.fc3(x),dim = 1)) # dim = 1 sums the rows so they equal 1. I.e. each input. \n",
    "\n",
    "def train_model(train_loader, valid_loader, model, epochs ,device = device, verbose = True):\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.9, 0.999)) # Just an Adam Optimiser\n",
    "    \n",
    "    # Training variables\n",
    "    train_size = len(train_loader.dataset)\n",
    "    train_num_batches = len(train_loader)\n",
    "    \n",
    "    # validation variables\n",
    "    valid_size = len(valid_loader.dataset)\n",
    "    num_batches = len(valid_loader)\n",
    "    \n",
    "    \n",
    "    for t in range(epochs):\n",
    "        correct = 0\n",
    "        avg_valid_loss, valid_correct = 0, 0\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            # Store loss history for future plotting\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        history['avg_train_loss'].append(loss.item())\n",
    "        avg_train_loss = loss / train_num_batches\n",
    "        accuracy = correct / train_size * 100           \n",
    "        history['train_accuracy'].append(accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Epoch {t+1} of {epochs}\")\n",
    "            print('-' * 15)\n",
    "            print(f\"Training Results, Epoch {t+1}:\\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "\n",
    "              ###################### VALIDATION LOOP ##############################\n",
    "        with torch.no_grad():\n",
    "            for valid_X, valid_y in valid_loader:\n",
    "                valid_X = valid_X.to(device)\n",
    "                valid_y = valid_y.to(device)\n",
    "\n",
    "                valid_pred = model(valid_X)\n",
    "                valid_loss = loss_fn(valid_pred, valid_y).item()\n",
    "                avg_valid_loss += loss_fn(valid_pred, valid_y).item()\n",
    "                valid_correct += (valid_pred.argmax(1) == valid_y).type(torch.float).sum().item()\n",
    "              \n",
    "        avg_valid_loss /= num_batches\n",
    "        valid_accuracy = valid_correct / valid_size * 100\n",
    "              \n",
    "        history['avg_valid_loss'].append(avg_valid_loss)\n",
    "        history['valid_accuracy'].append(valid_accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Epoch {t+1} of {epochs}\")\n",
    "            print('-' * 15)\n",
    "            print(f\"Validation Results, Epoch {t+1}: \\n Accuracy: {(valid_accuracy):>0.1f}%, Avg loss: {avg_valid_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Final Train Accuracy: {(accuracy):>0.1f}%, and Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "    print(f\"Final Validation Accuracy: {(valid_accuracy):>0.1f}%, and Avg loss: {avg_valid_loss:>8f} \\n\")\n",
    "    return history\n",
    "\n",
    "def get_ann_results(resolution, epochs = 20, slope = 25, loss_upper = 1.05, acc_lower = 0, acc_higher = 100, verbose = True):\n",
    "    train, valid, test = load_in_data(resolution)\n",
    "    model = Ann_Net(resolution).to(device)\n",
    "\n",
    "    output = train_model(train,valid,model,epochs, verbose = verbose)\n",
    "    plot_training_history(output,resolution, ylimita = loss_upper, ylimitb_lower = acc_lower, ylimitb_upper = acc_higher)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_in_data(res, ratio = 1):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((res, res)), #Resize images to 28*28\n",
    "        transforms.Grayscale(), # Make sure image is grayscale\n",
    "        transforms.ToTensor()]) # change each image array to a tensor which automatically scales inputs to [0,1]\n",
    "\n",
    "    mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # Download training set and apply transformations. \n",
    "    mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform) # same for test set\n",
    "\n",
    "    train_len = int(len(mnist_train)/ratio)\n",
    "    dummy_len = len(mnist_train) - train_len\n",
    "    train_dataset, _ = random_split(mnist_train, (train_len, dummy_len), generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # Load the data into the DataLoader so it's passed through the model, shuffled in batches. \n",
    "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def output_formula(input_size, filter_size, padding, stride):\n",
    "    formula = math.floor(((((input_size - filter_size + 2*padding)/stride) + 1)))\n",
    "    \n",
    "    return formula \n",
    "\n",
    "def all_output_sizes(res, conv_filter = 3, conv_padding = 1, conv_stride = 1, mp_filter = 3, mp_padding = 0, mp_stride = 2):\n",
    "    \n",
    "    conv1 = output_formula(res, conv_filter, conv_padding, conv_stride)   # Output size from applying conv1 to input \n",
    "    mp1 = output_formula(conv1, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 1 to conv1 \n",
    "    \n",
    "    conv2 = output_formula(mp1, conv_filter, conv_padding, conv_stride)   # Output size from applying conv2 to max pooling 1\n",
    "    conv3 = output_formula(conv2, conv_filter, conv_padding, conv_stride) # Output size from applying conv3 to conv 2\n",
    "    mp2 = output_formula(conv3, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 2 to conv3\n",
    "    \n",
    "    conv4 = output_formula(mp2, conv_filter, conv_padding, conv_stride)   # Output size from applying conv 4 to max pooling 2\n",
    "    conv5 = output_formula(conv4, conv_filter, conv_padding, conv_stride) # Output size from applying conv5 to conv 4\n",
    "    mp3 = output_formula(conv5, mp_filter, mp_padding, mp_stride)         # Output size from applying max pooling 3 to conv 5\n",
    "    \n",
    "    outputs_I_need = [mp1, conv2, mp2, conv4, mp3]\n",
    "    \n",
    "    return outputs_I_need\n",
    "\n",
    "def plot_training_history(history, res, loss_upper = 1.05, acc_lower = -0.05, acc_higher = 105):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    \n",
    "    ax1.plot(history['avg_train_loss'], label='train loss',marker = 'o')\n",
    "    ax1.plot(history['avg_valid_loss'], label='validation loss',marker = 'o')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax1.set_ylim([-0.05, loss_upper])\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Loss',fontsize = 16)\n",
    "    ax1.set_xlabel('Epoch',fontsize = 16)\n",
    "    \n",
    "    ax2.plot(history['train_accuracy'], label='train accuracy',marker = 'o')\n",
    "    ax2.plot(history['valid_accuracy'], label='validation accuracy',marker = 'o')\n",
    "\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2.set_ylim([acc_lower, acc_higher])\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    ax2.set_ylabel('Accuracy',fontsize = 16)\n",
    "    ax2.yaxis.set_major_formatter(PercentFormatter(100))\n",
    "    ax2.set_xlabel('Epoch',fontsize = 16)\n",
    "    fig.suptitle(f'Training history ({res}*{res})',fontsize = 20)\n",
    "    plt.show()\n",
    "\n",
    "def store_best_results(history):\n",
    "    # Want to take the last entry from each output(best results) and store them all in a Dataframe\n",
    "    placeholder = []\n",
    "    placeholder.append(history['avg_train_loss'][-1])\n",
    "    placeholder.append(history['train_accuracy'][-1])\n",
    "    placeholder.append(history['avg_valid_loss'][-1])\n",
    "    placeholder.append(history['valid_accuracy'][-1])\n",
    "    \n",
    "    return placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def put_results_in_df(output):\n",
    "    df = pd.DataFrame()\n",
    "    df['avg_train_loss'] = output['avg_train_loss']\n",
    "    df['train_accuracy'] = output['train_accuracy']\n",
    "    df['avg_valid_loss'] = output['avg_valid_loss']\n",
    "    df['valid_accuracy'] = output['valid_accuracy']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Ann_Net(nn.Module):\n",
    "    def __init__(self,res):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(res*res, num_hidden)     # input layer with as many neurons as pixels. \n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)    # Second Dense/linear layer that receives the output spikes from previous layer\n",
    "        self.fc3 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.fc4 = nn.Linear(num_hidden, num_outputs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        return(F.log_softmax(self.fc4(x),dim = 1)) # dim = 1 sums the rows so they equal 1. I.e. each input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader, model, epochs ,device = device, verbose = True):\n",
    "    start_time = time.time()\n",
    "    print('Starting Training')\n",
    "    history = defaultdict(list)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999)) # Just an Adam Optimiser\n",
    "    \n",
    "    # Training variables\n",
    "    train_size = len(train_loader.dataset)\n",
    "    train_num_batches = len(train_loader)\n",
    "    \n",
    "    # validation variables\n",
    "    valid_size = len(valid_loader.dataset)\n",
    "    num_batches = len(valid_loader)\n",
    "    \n",
    "    \n",
    "    for t in range(epochs):\n",
    "        correct = 0\n",
    "        avg_valid_loss, valid_correct = 0, 0\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            # Store loss history for future plotting\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        history['avg_train_loss'].append(loss.item())\n",
    "        avg_train_loss = loss / train_num_batches\n",
    "        accuracy = correct / train_size * 100           \n",
    "        history['train_accuracy'].append(accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Epoch {t+1} of {epochs}\")\n",
    "            print('-' * 15)\n",
    "            print(f\"Training Results, Epoch {t+1}:\\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "\n",
    "              ###################### VALIDATION LOOP ##############################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for valid_X, valid_y in valid_loader:\n",
    "                valid_X = valid_X.to(device)\n",
    "                valid_y = valid_y.to(device)\n",
    "\n",
    "                valid_pred = model(valid_X)\n",
    "                valid_loss = loss_fn(valid_pred, valid_y).item()\n",
    "                avg_valid_loss += loss_fn(valid_pred, valid_y).item()\n",
    "                valid_correct += (valid_pred.argmax(1) == valid_y).type(torch.float).sum().item()\n",
    "              \n",
    "        avg_valid_loss /= num_batches\n",
    "        valid_accuracy = valid_correct / valid_size * 100\n",
    "              \n",
    "        history['avg_valid_loss'].append(avg_valid_loss)\n",
    "        history['valid_accuracy'].append(valid_accuracy)\n",
    "        \n",
    "        if verbose == True: \n",
    "            print(f\"Epoch {t+1} of {epochs}\")\n",
    "            print('-' * 15)\n",
    "            print(f\"Validation Results, Epoch {t+1}: \\n Accuracy: {(valid_accuracy):>0.1f}%, Avg loss: {avg_valid_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Final Train Accuracy: {(accuracy):>0.1f}%, and Avg loss: {avg_train_loss.item():>8f} \\n\")\n",
    "    print(f\"Final Validation Accuracy: {(valid_accuracy):>0.1f}%, and Avg loss: {avg_valid_loss:>8f} \\n\")\n",
    "    current_time = time.time()\n",
    "    total = current_time - start_time\n",
    "    print(f'Training time: {round(total/60,2)} minutes')\n",
    "    return history\n",
    "\n",
    "def get_ann_results(resolution, ratio = 1,  epochs = 20, verbose = True):\n",
    "    train, valid = load_in_data(resolution, ratio)\n",
    "    model = Ann_Net(resolution).to(device)\n",
    "\n",
    "    output = train_model(train,valid,model,epochs, verbose = verbose)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56 * 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 98.5%, and Avg loss: 0.152750 \n",
      "\n",
      "Training time: 30.77 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_56_r1 = get_ann_results(resolution = 56, ratio = 1, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 97.3%, and Avg loss: 0.200885 \n",
      "\n",
      "Training time: 12.52 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_56_r4 = get_ann_results(resolution = 56, ratio = 4, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 96.3%, and Avg loss: 0.328868 \n",
      "\n",
      "Training time: 23.44 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_56_r10 = get_ann_results(resolution = 56, ratio = 10, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 96.0%, and Avg loss: 0.000004 \n",
      "\n",
      "Final Validation Accuracy: 87.9%, and Avg loss: 0.757518 \n",
      "\n",
      "Training time: 11.8 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_56_r100 = get_ann_results(resolution = 56, ratio = 100, epochs = 75,  verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 98.4%, and Avg loss: 0.141690 \n",
      "\n",
      "Training time: 18.64 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_28_r1 = get_ann_results(resolution = 28, ratio = 1, epochs = 75,  verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 96.8%, and Avg loss: 0.247540 \n",
      "\n",
      "Training time: 4.37 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_28_r4 = get_ann_results(resolution = 28, ratio = 4, epochs = 75,  verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 95.3%, and Avg loss: 0.430565 \n",
      "\n",
      "Training time: 6.18 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_28_r10 = get_ann_results(resolution = 28, ratio = 10, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 96.0%, and Avg loss: 0.000012 \n",
      "\n",
      "Final Validation Accuracy: 86.7%, and Avg loss: 0.790981 \n",
      "\n",
      "Training time: 3.21 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_28_r100 = get_ann_results(resolution = 28, ratio = 100, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 * 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.9%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 98.3%, and Avg loss: 0.116023 \n",
      "\n",
      "Training time: 20.8 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "output_14_r1 = get_ann_results(resolution = 14, ratio = 1, epochs = 75,  verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.8%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 97.3%, and Avg loss: 0.180026 \n",
      "\n",
      "Training time: 5.19 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_14_r4 = get_ann_results(resolution = 14, ratio = 4, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.000000 \n",
      "\n",
      "Final Validation Accuracy: 96.3%, and Avg loss: 0.259648 \n",
      "\n",
      "Training time: 4.87 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_14_r10 = get_ann_results(resolution = 14, ratio = 10, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 96.0%, and Avg loss: 0.000039 \n",
      "\n",
      "Final Validation Accuracy: 88.6%, and Avg loss: 0.672056 \n",
      "\n",
      "Training time: 2.89 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_14_r100 = get_ann_results(resolution = 14, ratio = 100, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.000002 \n",
      "\n",
      "Final Validation Accuracy: 97.8%, and Avg loss: 0.115469 \n",
      "\n",
      "Training time: 14.9 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_7_r1 = get_ann_results(resolution = 7, ratio = 1, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.7%, and Avg loss: 0.000078 \n",
      "\n",
      "Final Validation Accuracy: 96.2%, and Avg loss: 0.173870 \n",
      "\n",
      "Training time: 6.09 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "output_7_r4 = get_ann_results(resolution = 7, ratio = 4, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 99.5%, and Avg loss: 0.000083 \n",
      "\n",
      "Final Validation Accuracy: 94.9%, and Avg loss: 0.297207 \n",
      "\n",
      "Training time: 9.04 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_7_r10 = get_ann_results(resolution = 7, ratio = 10, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Done!\n",
      "Final Train Accuracy: 95.7%, and Avg loss: 0.000246 \n",
      "\n",
      "Final Validation Accuracy: 87.7%, and Avg loss: 0.624996 \n",
      "\n",
      "Training time: 4.71 minutes\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "output_7_r100 = get_ann_results(resolution = 7, ratio = 100, epochs = 75, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_res = ['output_56', 'output_28', 'output_14','output_7']\n",
    "output_ratio = ['_r1','_r4','_r10','_r100']\n",
    "index = ['avg_train_loss', 'train_accuracy', 'avg_valid_loss', 'valid_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = []\n",
    "all_models = []\n",
    "for name in output_res:\n",
    "    for ratio in output_ratio: \n",
    "        model_name = name + ratio\n",
    "        all_models.append(model_name)\n",
    "        for indice in index: \n",
    "            column_name = name + ratio + '_' + indice\n",
    "            all_columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for entry in all_models:\n",
    "    for key in index:\n",
    "        string = entry + '_' + key\n",
    "        \n",
    "        df[string] = locals()[entry][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_simple_ann_training_histories_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masters-Project-uJcOuMmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "42dcfe4d3ff455891dcc0edcfbcf5ccac60cdc0dc8dd9e15fc951f2fbb26b4f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
